# scripts/analyze_and_update.py
# -*- coding: utf-8 -*-
from __future__ import annotations

import os, re, sys, json, time, random
from pathlib import Path
from datetime import datetime, date
from typing import Dict, List, Tuple, Optional

import pandas as pd
import gspread
from google.oauth2.service_account import Credentials
from gspread.exceptions import APIError

# ===================== ì„¤ì • =====================
LOG_DIR = Path("analyze_report")
ARTIFACTS_DIR = os.environ.get("ARTIFACTS_DIR", "artifacts")

SUMMARY_SHEET_NAME = "ê±°ë˜ìš”ì•½"

# ê±°ë˜ìš”ì•½ì— ê¸°ë¡í•  ì—´ (ì‹œíŠ¸ì— ìˆëŠ” ì—´ë§Œ ì“°ë¯€ë¡œ ë§ì•„ë„ ì•ˆì „)
SUMMARY_COLS = [
    "ì „êµ­","ì„œìš¸",
    "ê°•ë‚¨êµ¬","ì••êµ¬ì •ë™",
    "ê²½ê¸°ë„","ì¸ì²œê´‘ì—­ì‹œ","ì„¸ì¢…ì‹œ","ìš¸ì‚°",
    "ì„œì´ˆêµ¬","ì†¡íŒŒêµ¬","ìš©ì‚°êµ¬","ê°•ë™êµ¬","ì„±ë™êµ¬","ë§ˆí¬êµ¬","ì–‘ì²œêµ¬","ë™ì‘êµ¬","ì˜ë“±í¬êµ¬","ì¢…ë¡œêµ¬","ê´‘ì§„êµ¬",
    "ê°•ì„œêµ¬","ê°•ë¶êµ¬","ê´€ì•…êµ¬","êµ¬ë¡œêµ¬","ê¸ˆì²œêµ¬","ë„ë´‰êµ¬","ë…¸ì›êµ¬",
    "ë™ëŒ€ë¬¸êµ¬","ì„œëŒ€ë¬¸êµ¬","ì„±ë¶êµ¬","ì€í‰êµ¬","ì¤‘êµ¬","ì¤‘ë‘êµ¬",
    "ë¶€ì‚°","ëŒ€êµ¬","ê´‘ì£¼","ëŒ€ì „","ê°•ì›ë„","ê²½ë‚¨","ê²½ë¶","ì „ë‚¨","ì „ë¶","ì¶©ë‚¨","ì¶©ë¶","ì œì£¼"
]

# ì •ì‹â†’ìš”ì•½(ì¶•ì•½) ë§¤í•‘(ì§‘ê³„ í‚¤)
PROV_MAP = {
    "ì„œìš¸íŠ¹ë³„ì‹œ":"ì„œìš¸",
    "ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ":"ì„¸ì¢…ì‹œ",
    "ê°•ì›íŠ¹ë³„ìì¹˜ë„":"ê°•ì›ë„",
    "ê²½ê¸°ë„":"ê²½ê¸°ë„",
    "ì¸ì²œê´‘ì—­ì‹œ":"ì¸ì²œê´‘ì—­ì‹œ",
    "ë¶€ì‚°ê´‘ì—­ì‹œ":"ë¶€ì‚°",
    "ëŒ€êµ¬ê´‘ì—­ì‹œ":"ëŒ€êµ¬",
    "ê´‘ì£¼ê´‘ì—­ì‹œ":"ê´‘ì£¼",
    "ëŒ€ì „ê´‘ì—­ì‹œ":"ëŒ€ì „",
    "ìš¸ì‚°ê´‘ì—­ì‹œ":"ìš¸ì‚°",
    "ì „ë¼ë‚¨ë„":"ì „ë‚¨",
    "ì „ë¶íŠ¹ë³„ìì¹˜ë„":"ì „ë¶",
    "ê²½ìƒë‚¨ë„":"ê²½ë‚¨",
    "ê²½ìƒë¶ë„":"ê²½ë¶",
    "ì¶©ì²­ë‚¨ë„":"ì¶©ë‚¨",
    "ì¶©ì²­ë¶ë„":"ì¶©ë¶",
    "ì œì£¼íŠ¹ë³„ìì¹˜ë„":"ì œì£¼",
}

# ğŸ”¸ ì •ê·œí™” ìœ í‹¸
def ns(s: str) -> str:
    # ëª¨ë“  ê³µë°±(ì „ê° í¬í•¨) ì œê±° + ì†Œë¬¸ìí™”(í•œê¸€ ì˜í–¥ ì—†ìŒ)
    return re.sub(r"\s+", "", str(s or "").replace("\u00A0","")).strip().lower()

# ğŸ”¸ ê´‘ì—­ëª… ëª¨ë“  ë³€í˜•ì„ í•˜ë‚˜ì˜ ì§‘ê³„í‚¤ë¡œ ìºë…¼í™”
def build_prov_canon() -> Dict[str, str]:
    canon: Dict[str, str] = {}
    # 1) ì •ì‹ëª… â†’ ì¶•ì•½í‚¤
    for k, v in PROV_MAP.items():
        canon[ns(k)] = v
    # 2) ì¶•ì•½ëª…ì´ ê·¸ëŒ€ë¡œ ë“¤ì–´ì˜¤ëŠ” íŒŒì¼ë„ ìˆìœ¼ë‹ˆ ê·¸ëŒ€ë¡œ í—ˆìš©
    for k in SUMMARY_COLS:
        canon[ns(k)] = k
    # 3) í”í•œ ë³€í˜•(ì„¸ì¢…) ë³´ê°•
    canon[ns("ì„¸ì¢…")] = "ì„¸ì¢…ì‹œ"
    canon[ns("ìš¸ì‚°ì‹œ")] = "ìš¸ì‚°"
    canon[ns("ë¶€ì‚°ì‹œ")] = "ë¶€ì‚°"
    canon[ns("ëŒ€ì „ì‹œ")] = "ëŒ€ì „"
    canon[ns("ëŒ€êµ¬ì‹œ")] = "ëŒ€êµ¬"
    canon[ns("ê´‘ì£¼ì‹œ")] = "ê´‘ì£¼"
    return canon

PROV_CANON = build_prov_canon()

# ì••êµ¬ì •ë™ ë³¸í‘œ ê³ ì • ì—´(ì›ë³¸ ê·¸ëŒ€ë¡œ)
APGU_BASE_COLS = [
    "ê´‘ì—­","êµ¬","ë²•ì •ë™","ë¦¬","ë²ˆì§€","ë³¸ë²ˆ","ë¶€ë²ˆ","ë‹¨ì§€ëª…","ì „ìš©ë©´ì (ã¡)",
    "ê³„ì•½ë…„","ê³„ì•½ì›”","ê³„ì•½ì¼","ê±°ë˜ê¸ˆì•¡(ë§Œì›)","ë™","ì¸µ",
    "ë§¤ìˆ˜ì","ë§¤ë„ì","ê±´ì¶•ë…„ë„","ë„ë¡œëª…","í•´ì œì‚¬ìœ ë°œìƒì¼","ê±°ë˜ìœ í˜•",
    "ì¤‘ê°œì‚¬ì†Œì¬ì§€","ë“±ê¸°ì¼ì","ì£¼íƒìœ í˜•"
]

# ===================== ë¡œê¹…/ë¦¬íŠ¸ë¼ì´ =====================
def _ensure_logdir():
    try:
        if LOG_DIR.exists() and not LOG_DIR.is_dir():
            LOG_DIR.unlink()
        LOG_DIR.mkdir(parents=True, exist_ok=True)
    except Exception:
        pass

_ensure_logdir()
RUN_LOG = LOG_DIR / "latest.log"

def log(msg: str):
    line = f"[{datetime.now().strftime('%H:%M:%S')}] {msg}"
    print(line)
    try:
        with RUN_LOG.open("a", encoding="utf-8") as f:
            f.write(line+"\n")
    except Exception:
        pass

_LAST = 0.0
def _throttle(sec=0.35):
    import time as _t
    global _LAST
    now = _t.time()
    if now - _LAST < sec:
        _t.sleep(sec - (now - _LAST))
    _LAST = _t.time()

def _retry(fn, *a, **kw):
    base = 0.8
    for i in range(7):
        try:
            _throttle()
            return fn(*a, **kw)
        except APIError as e:
            s = str(e)
            if any(x in s for x in ("429","500","502","503")):
                time.sleep(base*(2**i) + random.uniform(0,0.25))
                continue
            raise

# ===================== gspread í—¬í¼ =====================
def a1_col(n: int) -> str:
    s = ""
    while n > 0:
        n, r = divmod(n-1, 26)
        s = chr(65+r) + s
    return s

def values_batch_update(ws: gspread.Worksheet, data: List[Dict]):
    body = {"valueInputOption": "USER_ENTERED", "data": data}
    return _retry(ws.spreadsheet.values_batch_update, body=body)

def batch_format(ws: gspread.Worksheet, requests: List[dict]):
    if not requests: return
    _retry(ws.spreadsheet.batch_update, {"requests": requests})

def fuzzy_ws(sh: gspread.Spreadsheet, wanted: str) -> Optional[gspread.Worksheet]:
    tgt = re.sub(r"\s+","", wanted)
    for ws in sh.worksheets():
        if re.sub(r"\s+","", ws.title) == tgt:
            log(f"[ws] matched: '{ws.title}' (wanted='{wanted}')")
            return ws
    return None

# ===================== íŒŒì¼/ì½ê¸° =====================
def read_month_df(path: Path) -> pd.DataFrame:
    df = pd.read_excel(path, sheet_name="data", dtype=str)
    df = df.fillna("")
    # ìˆ«ìí˜• ìºìŠ¤íŒ…(ì›ë³¸ ì»¬ëŸ¼ì€ ìœ ì§€)
    for c in ["ê³„ì•½ë…„","ê³„ì•½ì›”","ê³„ì•½ì¼","ê±°ë˜ê¸ˆì•¡(ë§Œì›)"]:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")

    # ğŸ”¸ ì •ê·œí™” ì „ìš© ì»¬ëŸ¼ ì¶”ê°€(ì›ë³¸ì€ ê·¸ëŒ€ë¡œ ë‘ )
    def _nscol(col):
        if col in df.columns:
            return df[col].astype(str).map(ns)
        return pd.Series([""]*len(df), dtype=str)

    df["_ê´‘ì—­_n"] = _nscol("ê´‘ì—­")
    df["_êµ¬_n"] = _nscol("êµ¬")
    df["_ë²•ì •ë™_n"] = _nscol("ë²•ì •ë™")

    return df

def eok_series(ser) -> pd.Series:
    s = pd.to_numeric(ser, errors="coerce").dropna()
    if s.empty: return pd.Series([], dtype=float)
    return s / 10000.0

def round2(v) -> str:
    try:
        return f"{float(v):.2f}"
    except Exception:
        return ""

# ===================== ì§‘ê³„ =====================
def agg_all_stats(df: pd.DataFrame):
    counts = {col:0 for col in SUMMARY_COLS}
    med = {col:"" for col in SUMMARY_COLS}
    mean = {col:"" for col in SUMMARY_COLS}

    all_eok = eok_series(df.get("ê±°ë˜ê¸ˆì•¡(ë§Œì›)", []))
    counts["ì „êµ­"] = int(len(df))
    if not all_eok.empty:
        med["ì „êµ­"] = round2(all_eok.median())
        mean["ì „êµ­"] = round2(all_eok.mean())

    # ğŸ”¸ ê´‘ì—­ ì§‘ê³„: ì •ê·œí™” ì»¬ëŸ¼ê³¼ ìºë…¼ ë§¤í•‘ ì‚¬ìš©
    if "_ê´‘ì—­_n" in df.columns:
        for key_norm, sub in df.groupby("_ê´‘ì—­_n"):
            prov_key = PROV_CANON.get(key_norm, None)
            if prov_key and prov_key in counts:
                counts[prov_key] += int(len(sub))
                s = eok_series(sub.get("ê±°ë˜ê¸ˆì•¡(ë§Œì›)", []))
                if not s.empty:
                    med[prov_key] = round2(s.median())
                    mean[prov_key] = round2(s.mean())

    # ğŸ”¸ ì„œìš¸Â·êµ¬Â·ì••êµ¬ì •ë™ í•„í„°ë„ ì •ê·œí™” ì»¬ëŸ¼ ê¸°ë°˜
    seoul = df[df["_ê´‘ì—­_n"] == ns("ì„œìš¸íŠ¹ë³„ì‹œ")].copy()
    counts["ì„œìš¸"] = int(len(seoul))
    if len(seoul)>0:
        s = eok_series(seoul["ê±°ë˜ê¸ˆì•¡(ë§Œì›)"])
        if not s.empty:
            med["ì„œìš¸"] = round2(s.median())
            mean["ì„œìš¸"] = round2(s.mean())

    if "_êµ¬_n" in seoul.columns:
        for gu_key, sub in seoul.groupby("_êµ¬_n"):
            # êµ¬ëŠ” SUMMARY_COLSì— ì‹¤ì œ í‘œê¸°(ê³µë°± ìœ )ë¡œ í‚¤ê°€ ìˆìŒ â†’ ì—­íƒìƒ‰ í…Œì´ë¸”
            # ì •ê·œí™”ê°’ê³¼ ê°™ì€ ê²ƒì„ SUMMARY_COLSì—ì„œ ì°¾ì•„ ë§¤ì¹­
            target = None
            for name in SUMMARY_COLS:
                if ns(name) == gu_key:
                    target = name
                    break
            if target:
                counts[target] += int(len(sub))
                s = eok_series(sub["ê±°ë˜ê¸ˆì•¡(ë§Œì›)"])
                if not s.empty:
                    med[target] = round2(s.median())
                    mean[target] = round2(s.mean())

    ap = seoul[seoul["_ë²•ì •ë™_n"] == ns("ì••êµ¬ì •ë™")]
    counts["ì••êµ¬ì •ë™"] = int(len(ap))
    if len(ap)>0:
        s = eok_series(ap["ê±°ë˜ê¸ˆì•¡(ë§Œì›)"])
        med["ì••êµ¬ì •ë™"] = round2(s.median())
        mean["ì••êµ¬ì •ë™"] = round2(s.mean())

    return counts, med, mean

# ===================== íƒ­ ê¸°ë¡ (ì „êµ­/ì„œìš¸) =====================
def kdate(d: datetime) -> str:
    return f"{d.year}. {d.month}. {d.day}"

def find_or_append_date_row(ws: gspread.Worksheet, date_label: str) -> int:
    vals = _retry(ws.get_all_values)
    if not vals:
        return 2
    for i, row in enumerate(vals[1:], start=2):
        if (len(row)>0) and str(row[0]).strip()==date_label:
            return i
    return len(vals)+1

def header_to_count_key_map(header: List[str]) -> Dict[int, Optional[str]]:
    """ì‹œíŠ¸ í—¤ë”ë¥¼ counts í‚¤ë¡œ ì—­ë§¤í•‘"""
    # PROV_CANONì˜ ì—­ì—­í• : ë‹¤ì–‘í•œ í‘œê¸°ë¥¼ countsí‚¤ë¡œ
    valid_count_keys = {ns(k): k for k in SUMMARY_COLS}

    out: Dict[int, Optional[str]] = {}
    for idx, h in enumerate(header, start=1):
        HH = str(h).strip()
        nH = ns(HH)
        if not nH:
            out[idx] = None; continue
        if HH in ("ë‚ ì§œ", "ë…„ì›”", "êµ¬ë¶„"):
            out[idx] = None; continue
        if HH in ("ì´í•©ê³„","í•©ê³„"):
            out[idx] = "__TOTAL__"; continue

        # 1) êµ¬ í—¤ë”(ê°•ë‚¨êµ¬ ë“±)
        if nH in valid_count_keys:
            out[idx] = valid_count_keys[nH]; continue
        # 2) ê´‘ì—­ í—¤ë”(ë¶€ì‚°ê´‘ì—­ì‹œ ë“±)
        if nH in PROV_CANON and PROV_CANON[nH] in SUMMARY_COLS:
            out[idx] = PROV_CANON[nH]; continue

        out[idx] = None
    return out

def write_month_sheet(ws, date_label: str, header: List[str], counts_map: Dict[str,int], total_from: str):
    col_map = header_to_count_key_map(header)
    row_idx = find_or_append_date_row(ws, date_label)

    payload = [{"range": f"A{row_idx}", "values": [[date_label]]}]

    for idx, key in col_map.items():
        if key is None:
            continue
        if key == "__TOTAL__":
            v = counts_map.get(total_from, None)
            if v is None: 
                continue
            payload.append({"range": f"{a1_col(idx)}{row_idx}", "values": [[int(v)]]})
        else:
            v = counts_map.get(key, None)
            if v is None: 
                continue
            payload.append({"range": f"{a1_col(idx)}{row_idx}", "values": [[int(v)]]})

    if payload:
        values_batch_update(ws, payload)
        log(f"[ws] {ws.title} -> {date_label} row={row_idx}")

# ===================== ê±°ë˜ìš”ì•½ =====================
def ym_from_filename(fname: str):
    m = re.search(r"(\d{2})(\d{2})_", fname)
    if not m: return None, None, None
    yy, mm = m.group(1), int(m.group(2))
    return f"ì „êµ­ 20{yy}ë…„ {mm}ì›”", f"ì„œìš¸ 20{yy}ë…„ {mm}ì›”", f"{yy}/{mm}"

def prev_ym(ym: str) -> str:
    yy, mm = ym.split("/")
    y = int(yy); m = int(mm)
    if m == 1:
        return f"{y-1}/12"
    return f"{yy}/{m-1}"

def find_summary_row(ws, ym: str, label: str) -> int:
    vals = _retry(ws.get_all_values)
    if not vals:
        return 2
    for i, row in enumerate(vals[1:], start=2):
        a = str(row[0]).strip() if len(row)>0 else ""
        b = str(row[1]).strip() if len(row)>1 else ""
        if a==ym and b==label:
            return i
    return len(vals)+1

def put_summary_line(ws, row_idx: int, ym: str, label: str, line_map: dict):
    header = _retry(ws.row_values, 1)
    if not header:
        _retry(ws.update, [["ë…„ì›”","êµ¬ë¶„"] + SUMMARY_COLS], "A1")
        header = ["ë…„ì›”","êµ¬ë¶„"] + SUMMARY_COLS
    hmap = {h:i+1 for i,h in enumerate(header)}
    payload = [
        {"range": f"A{row_idx}", "values": [[ym]]},
        {"range": f"B{row_idx}", "values": [[label]]},
    ]
    for c in SUMMARY_COLS:
        if c in hmap:
            payload.append({"range": f"{a1_col(hmap[c])}{row_idx}",
                            "values": [[line_map.get(c,"")]]})
    values_batch_update(ws, payload)

def color_diff_line(ws, row_idx: int, diff_line: dict, header: Optional[List[str]]) -> None:
    if not header:
        return
    hmap = {h: i + 1 for i, h in enumerate(header)}
    reqs = []
    for k, v in diff_line.items():
        if k not in hmap: continue
        if v == "" or v == "0": continue
        r, g, b = (0.0, 0.35, 1.0) if str(v).startswith("+") else (1.0, 0.0, 0.0)
        reqs.append({
            "repeatCell": {
                "range": {
                    "sheetId": ws.id,
                    "startRowIndex": row_idx - 1, "endRowIndex": row_idx,
                    "startColumnIndex": hmap[k] - 1, "endColumnIndex": hmap[k]
                },
                "cell": {"userEnteredFormat":{"textFormat":{"foregroundColor":{"red":r,"green":g,"blue":b}}}},
                "fields": "userEnteredFormat.textFormat.foregroundColor"
            }
        })
    batch_format(ws, reqs)

def write_month_summary(ws, y: int, m: int, counts: dict, med: dict, mean: dict, prev_counts: Optional[dict]):
    ym = f"{str(y%100).zfill(2)}/{m}"

    r1 = find_summary_row(ws, ym, "ê±°ë˜ê±´ìˆ˜")
    put_summary_line(ws, r1, ym, "ê±°ë˜ê±´ìˆ˜", counts)
    log(f"[summary] {ym} ê±°ë˜ê±´ìˆ˜ -> row={r1}")

    r2 = find_summary_row(ws, ym, "ì¤‘ì•™ê°’(ë‹¨ìœ„:ì–µ)")
    put_summary_line(ws, r2, ym, "ì¤‘ì•™ê°’(ë‹¨ìœ„:ì–µ)", med)
    log(f"[summary] {ym} ì¤‘ì•™ê°’ -> row={r2}")

    r3 = find_summary_row(ws, ym, "í‰ê· ê°€(ë‹¨ìœ„:ì–µ)")
    put_summary_line(ws, r3, ym, "í‰ê· ê°€(ë‹¨ìœ„:ì–µ)", mean)
    log(f"[summary] {ym} í‰ê· ê°€ -> row={r3}")

    if prev_counts:
        diffs = {}
        for c in SUMMARY_COLS:
            cur = int(counts.get(c,0) or 0)
            prv = int(prev_counts.get(c,0) or 0)
            d = cur - prv
            diffs[c] = f"+{d}" if d>0 else (str(d) if d<0 else "0")
    else:
        diffs = {c:"" for c in SUMMARY_COLS}
    r4 = find_summary_row(ws, ym, "ì „ì›”ëŒ€ë¹„ ê±´ìˆ˜ì¦ê°")
    put_summary_line(ws, r4, ym, "ì „ì›”ëŒ€ë¹„ ê±´ìˆ˜ì¦ê°", diffs)
    header = _retry(ws.row_values, 1) or []
    color_diff_line(ws, r4, diffs, header)
    log(f"[summary] {ym} ì „ì›”ëŒ€ë¹„ -> row={r4}")

# ===================== ì••êµ¬ì •ë™ (ì›ë³¸ ê·¸ëŒ€ë¡œ + ë³€ë™ìš”ì•½) =====================
def _apgu_norm(v) -> str:
    return "" if v is None else str(v).strip()

def _apgu_key_from_row_values(values: List[str], header: List[str]) -> str:
    idx = {h:i for i,h in enumerate(header)}
    parts = []
    for h in APGU_BASE_COLS:
        i = idx.get(h, None)
        parts.append(_apgu_norm(values[i] if (i is not None and i < len(values)) else ""))
    return "|".join(parts)

def _ensure_rows(ws: gspread.Worksheet, need_end_row: int):
    if need_end_row > ws.row_count:
        _retry(ws.add_rows, need_end_row - ws.row_count)

def fmt_kdate(d: date) -> str:
    return f"{d.year}. {d.month}. {d.day}"

def upsert_apgu_verbatim(ws: gspread.Worksheet, df_all: pd.DataFrame, run_day: date):
    df = df_all[df_all.get("_ë²•ì •ë™_n","") == ns("ì••êµ¬ì •ë™")].copy()
    if df.empty:
        log("[ì••êµ¬ì •ë™] no rows")
        return

    # í•„ìˆ˜ ì»¬ëŸ¼ ì±„ìš°ê¸°
    for c in APGU_BASE_COLS:
        if c not in df.columns:
            df[c] = ""

    # ì˜¤ë˜ëœ â†’ ìµœì‹ 
    for c in ["ê³„ì•½ë…„","ê³„ì•½ì›”","ê³„ì•½ì¼"]:
        if c not in df.columns:
            df[c] = ""
    df = df.sort_values(["ê³„ì•½ë…„","ê³„ì•½ì›”","ê³„ì•½ì¼"], ascending=[True,True,True], kind="mergesort")

    # í—¤ë” ê³ ì •
    vals = _retry(ws.get_all_values) or []
    if not vals:
        _retry(ws.update, [APGU_BASE_COLS], "A1")
        vals = [APGU_BASE_COLS]
    header = vals[0]
    if header != APGU_BASE_COLS:
        _retry(ws.update, [APGU_BASE_COLS], "A1")
        header = APGU_BASE_COLS

    # ê¸°ì¡´ ë³¸í‘œ í–‰(ë³€ë™ ë¸”ë¡ ì œì™¸)
    all_now = _retry(ws.get_all_values) or [header]
    body = all_now[1:]
    base_rows_old: List[List[str]] = []
    for r in body:
        if r and r[0] in ("ë³€ê²½êµ¬ë¶„","(ì‹ ê·œ)","(ì‚­ì œ)"):
            break
        base_rows_old.append((r + [""]*len(header))[:len(header)])

    # ì˜¤ëŠ˜ ë³¸í‘œ (ì›ë³¸ ê·¸ëŒ€ë¡œ)
    base_rows_new: List[List[str]] = []
    for _, row in df.iterrows():
        base_rows_new.append([_apgu_norm(row.get(c, "")) for c in APGU_BASE_COLS])

    # ë³¸í‘œ ë®ì–´ì“°ê¸°
    start = 2
    end = start + len(base_rows_new) - 1
    if end < start: end = start
    _ensure_rows(ws, end)
    _retry(ws.update, base_rows_new, f"A{start}:{a1_col(len(header))}{end}")
    log(f"[ì••êµ¬ì •ë™] base rows written: {len(base_rows_new)}")

    # ì‹ ê·œ/ì‚­ì œ ë¹„êµ
    old_keys = {_apgu_key_from_row_values(r, header) for r in base_rows_old}
    new_keys = {_apgu_key_from_row_values(r, header) for r in base_rows_new}
    added_keys = sorted(list(new_keys - old_keys))
    removed_keys = sorted(list(old_keys - new_keys))

    if not added_keys and not removed_keys:
        log("[ì••êµ¬ì •ë™] changes: none")
        return

    def _rowmap(rows: List[List[str]]):
        m={}
        for r in rows:
            m[_apgu_key_from_row_values(r, header)] = r
        return m

    new_map = _rowmap(base_rows_new)
    old_map = _rowmap(base_rows_old)

    change_header = ["ë³€ê²½êµ¬ë¶„","ë³€ê²½ì¼"] + APGU_BASE_COLS
    change_rows: List[List[str]] = [change_header]
    today_str = fmt_kdate(run_day)
    for k in added_keys:
        change_rows.append(["(ì‹ ê·œ)", today_str] + new_map[k])
    for k in removed_keys:
        change_rows.append(["(ì‚­ì œ)", today_str] + old_map[k])

    start_chg = end + 1
    end_chg = start_chg + len(change_rows) - 1
    _ensure_rows(ws, end_chg)
    _retry(ws.update, change_rows, f"A{start_chg}:{a1_col(len(change_header))}{end_chg}")

    # ë¹¨ê°„ ê¸€ì”¨
    req = {
        "repeatCell": {
            "range": {
                "sheetId": ws.id,
                "startRowIndex": start_chg-1,
                "endRowIndex": end_chg,
                "startColumnIndex": 0,
                "endColumnIndex": len(change_header)
            },
            "cell": {"userEnteredFormat":{"textFormat":{"foregroundColor":{"red":1.0,"green":0.0,"blue":0.0}}}},
            "fields": "userEnteredFormat.textFormat.foregroundColor"
        }
    }
    batch_format(ws, [req])
    log(f"[ì••êµ¬ì •ë™] changes: ì‹ ê·œ={len(added_keys)} ì‚­ì œ={len(removed_keys)}")

# ===================== ë©”ì¸ =====================
def main():
    try: RUN_LOG.write_text("", encoding="utf-8")
    except Exception: pass

    log("[MAIN]")
    log(f"artifacts_dir={ARTIFACTS_DIR}")

    # ì¸ì¦
    sa_json = os.environ.get("SA_JSON","").strip()
    sa_path = os.environ.get("SA_PATH","sa.json")
    if sa_json:
        creds = Credentials.from_service_account_info(
            json.loads(sa_json),
            scopes=["https://www.googleapis.com/auth/spreadsheets","https://www.googleapis.com/auth/drive"],
        )
    else:
        if not Path(sa_path).exists():
            raise RuntimeError("service account not provided")
        creds = Credentials.from_service_account_file(
            sa_path,
            scopes=["https://www.googleapis.com/auth/spreadsheets","https://www.googleapis.com/auth/drive"],
        )
    gc = gspread.authorize(creds)
    sh = _retry(gc.open_by_key, os.environ.get("SHEET_ID","").strip())
    log("[gspread] spreadsheet opened")

    # íŒŒì¼ ìˆ˜ì§‘
    files = sorted(Path(ARTIFACTS_DIR).rglob("ì „êµ­ *.xlsx"))
    log(f"[collect] found {len(files)} xlsx files")

    month_cache = {}  # ym -> {counts, med, mean}
    today_label = kdate(datetime.now())
    run_day = datetime.now().date()
    apgu_all: List[pd.DataFrame] = []

    for p in files:
        nat_title, se_title, ym = ym_from_filename(p.name)
        if not ym: 
            continue
        log(f"[file] {p.name} -> nat='{nat_title}' / seoul='{se_title}' / ym={ym}")

        df = read_month_df(p)
        log(f"[read] rows={len(df)} cols={len(df.columns)}")

        counts, med, mean = agg_all_stats(df)
        month_cache[ym] = {"counts": counts, "med": med, "mean": mean}

        # ì›”ë³„ íƒ­ ê¸°ë¡ (ìˆëŠ” íƒ­ì—ë§Œ)
        ws_nat = fuzzy_ws(sh, nat_title)
        if ws_nat:
            header_nat = _retry(ws_nat.row_values, 1)
            write_month_sheet(ws_nat, today_label, header_nat, counts, total_from="ì „êµ­")

        ws_se = fuzzy_ws(sh, se_title)
        if ws_se:
            header_se = _retry(ws_se.row_values, 1)
            write_month_sheet(ws_se, today_label, header_se, counts, total_from="ì„œìš¸")

        # ì••êµ¬ì •ë™ ì›ë³¸ ëˆ„ì (ì •ê·œí™” í•„í„°)
        ap = df[(df.get("_ê´‘ì—­_n","") == ns("ì„œìš¸íŠ¹ë³„ì‹œ")) &
                (df.get("_ë²•ì •ë™_n","") == ns("ì••êµ¬ì •ë™"))].copy()
        if not ap.empty:
            apgu_all.append(ap)

    # ê±°ë˜ìš”ì•½
    ws_sum = fuzzy_ws(sh, SUMMARY_SHEET_NAME)
    if ws_sum and month_cache:
        def ym_key(ym): 
            yy, mm = ym.split("/")
            return (int(yy), int(mm))
        for ym in sorted(month_cache.keys(), key=ym_key):
            cur = month_cache[ym]
            prv = month_cache.get(prev_ym(ym))
            write_month_summary(
                ws_sum,
                2000 + int(ym.split("/")[0]),
                int(ym.split("/")[1]),
                cur["counts"], cur["med"], cur["mean"],
                prv["counts"] if prv else None
            )
            time.sleep(0.2)

    # ì••êµ¬ì •ë™ ì›ë³¸ ê·¸ëŒ€ë¡œ + ë³€ë™ìš”ì•½
    if apgu_all:
        ws_ap = fuzzy_ws(sh, "ì••êµ¬ì •ë™")
        if ws_ap:
            all_df = pd.concat(apgu_all, ignore_index=True)
            upsert_apgu_verbatim(ws_ap, all_df, run_day)
        else:
            log("[ì••êµ¬ì •ë™] sheet not found (skip)")

    log("[main] done")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        log(f"[ERROR] {repr(e)}")
        raise
