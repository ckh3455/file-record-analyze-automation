# scripts/analyze_and_update.py
# -*- coding: utf-8 -*-
from __future__ import annotations

import os, re, sys, json, time, random
from pathlib import Path
from datetime import datetime, date
from typing import Dict, List, Tuple, Optional

import pandas as pd
import gspread
from google.oauth2.service_account import Credentials
from gspread.exceptions import APIError

# ===================== ê¸°ë³¸ ì„¤ì • =====================
LOG_DIR = Path("analyze_report")
ARTIFACTS_DIR = os.environ.get("ARTIFACTS_DIR", "artifacts")

SUMMARY_SHEET_NAME = "ê±°ë˜ìš”ì•½"

# ê±°ë˜ìš”ì•½ì— ê¸°ë¡í•  ì—´ (ì‹œíŠ¸ì— ìˆëŠ” ì—´ë§Œ ì“°ë¯€ë¡œ ë§ì•„ë„ ì•ˆì „)
SUMMARY_COLS = [
    "ì „êµ­","ì„œìš¸",
    "ê°•ë‚¨êµ¬","ì••êµ¬ì •ë™",
    "ê²½ê¸°ë„","ì¸ì²œê´‘ì—­ì‹œ","ì„¸ì¢…ì‹œ","ìš¸ì‚°",
    "ì„œì´ˆêµ¬","ì†¡íŒŒêµ¬","ìš©ì‚°êµ¬","ê°•ë™êµ¬","ì„±ë™êµ¬","ë§ˆí¬êµ¬","ì–‘ì²œêµ¬","ë™ì‘êµ¬","ì˜ë“±í¬êµ¬","ì¢…ë¡œêµ¬","ê´‘ì§„êµ¬",
    "ê°•ì„œêµ¬","ê°•ë¶êµ¬","ê´€ì•…êµ¬","êµ¬ë¡œêµ¬","ê¸ˆì²œêµ¬","ë„ë´‰êµ¬","ë…¸ì›êµ¬",
    "ë™ëŒ€ë¬¸êµ¬","ì„œëŒ€ë¬¸êµ¬","ì„±ë¶êµ¬","ì€í‰êµ¬","ì¤‘êµ¬","ì¤‘ë‘êµ¬",
    "ë¶€ì‚°","ëŒ€êµ¬","ê´‘ì£¼","ëŒ€ì „","ê°•ì›ë„","ê²½ë‚¨","ê²½ë¶","ì „ë‚¨","ì „ë¶","ì¶©ë‚¨","ì¶©ë¶","ì œì£¼"
]

# ê´‘ì—­ í‘œê¸° ì •ê·œí™” â†’ ìš”ì•½ì—´/í—¤ë”ëª…
PROV_MAP = {
    "ì„œìš¸íŠ¹ë³„ì‹œ":"ì„œìš¸",
    "ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ":"ì„¸ì¢…ì‹œ",
    "ê°•ì›íŠ¹ë³„ìì¹˜ë„":"ê°•ì›ë„",
    "ê²½ê¸°ë„":"ê²½ê¸°ë„",
    "ì¸ì²œê´‘ì—­ì‹œ":"ì¸ì²œê´‘ì—­ì‹œ",
    "ë¶€ì‚°ê´‘ì—­ì‹œ":"ë¶€ì‚°",
    "ëŒ€êµ¬ê´‘ì—­ì‹œ":"ëŒ€êµ¬",
    "ê´‘ì£¼ê´‘ì—­ì‹œ":"ê´‘ì£¼",
    "ëŒ€ì „ê´‘ì—­ì‹œ":"ëŒ€ì „",
    "ìš¸ì‚°ê´‘ì—­ì‹œ":"ìš¸ì‚°",
    "ì „ë¼ë‚¨ë„":"ì „ë‚¨",
    "ì „ë¶íŠ¹ë³„ìì¹˜ë„":"ì „ë¶",
    "ê²½ìƒë‚¨ë„":"ê²½ë‚¨",
    "ê²½ìƒë¶ë„":"ê²½ë¶",
    "ì¶©ì²­ë‚¨ë„":"ì¶©ë‚¨",
    "ì¶©ì²­ë¶ë„":"ì¶©ë¶",
    "ì œì£¼íŠ¹ë³„ìì¹˜ë„":"ì œì£¼",
}

# ì••êµ¬ì •ë™ ë³¸í‘œ ê³ ì • ì—´(ì›ë³¸ ê·¸ëŒ€ë¡œ)
APGU_BASE_COLS = [
    "ê´‘ì—­","êµ¬","ë²•ì •ë™","ë¦¬","ë²ˆì§€","ë³¸ë²ˆ","ë¶€ë²ˆ","ë‹¨ì§€ëª…","ì „ìš©ë©´ì (ã¡)",
    "ê³„ì•½ë…„","ê³„ì•½ì›”","ê³„ì•½ì¼","ê±°ë˜ê¸ˆì•¡(ë§Œì›)","ë™","ì¸µ",
    "ë§¤ìˆ˜ì","ë§¤ë„ì","ê±´ì¶•ë…„ë„","ë„ë¡œëª…","í•´ì œì‚¬ìœ ë°œìƒì¼","ê±°ë˜ìœ í˜•",
    "ì¤‘ê°œì‚¬ì†Œì¬ì§€","ë“±ê¸°ì¼ì","ì£¼íƒìœ í˜•"
]

# ===================== ë¡œê¹…/ë¦¬íŠ¸ë¼ì´ =====================
def _ensure_logdir():
    try:
        if LOG_DIR.exists() and not LOG_DIR.is_dir():
            LOG_DIR.unlink()
        LOG_DIR.mkdir(parents=True, exist_ok=True)
    except Exception:
        pass

_ensure_logdir()
RUN_LOG = LOG_DIR / "latest.log"

def log(msg: str):
    line = f"[{datetime.now().strftime('%H:%M:%S')}] {msg}"
    print(line)
    try:
        with RUN_LOG.open("a", encoding="utf-8") as f:
            f.write(line+"\n")
    except Exception:
        pass

_LAST = 0.0
def _throttle(sec=0.35):
    global _LAST
    now = time.time()
    if now - _LAST < sec:
        time.sleep(sec - (now - _LAST))
    _LAST = time.time()

def _retry(fn, *a, **kw):
    base = 0.8
    for i in range(7):
        try:
            _throttle()
            return fn(*a, **kw)
        except APIError as e:
            s = str(e)
            if any(x in s for x in ("429","500","502","503")):
                time.sleep(base*(2**i) + random.uniform(0,0.25))
                continue
            raise

# ===================== gspread í—¬í¼ =====================
def a1_col(n: int) -> str:
    s = ""
    while n > 0:
        n, r = divmod(n-1, 26)
        s = chr(65+r) + s
    return s

def values_batch_update(ws: gspread.Worksheet, data: List[Dict]):
    body = {"valueInputOption": "USER_ENTERED", "data": data}
    return _retry(ws.spreadsheet.values_batch_update, body=body)

def batch_format(ws: gspread.Worksheet, requests: List[dict]):
    if not requests: return
    _retry(ws.spreadsheet.batch_update, {"requests": requests})

def fuzzy_ws(sh: gspread.Spreadsheet, wanted: str) -> Optional[gspread.Worksheet]:
    """
    íƒ­ ì œëª©ì´ 'ì „êµ­ 25ë…„ 6ì›”' ë˜ëŠ” 'ì „êµ­ 2025ë…„ 6ì›”' ë“± í˜¼ì¬í•´ë„ ê³µë°± ì œê±° ë¹„êµë¡œ ë§¤ì¹­
    """
    tgt = re.sub(r"\s+","", wanted)
    for ws in sh.worksheets():
        if re.sub(r"\s+","", ws.title) == tgt:
            log(f"[ws] matched (exact): '{ws.title}'")
            return ws
    # ê³µë°± ì œê±° ë™ì¼ í›„ë³´ ì¬íƒìƒ‰
    for ws in sh.worksheets():
        if re.sub(r"\s+","", ws.title) == re.sub(r"\s+","", wanted):
            log(f"[ws] matched (nospace): '{ws.title}' (wanted='{wanted}')")
            return ws
    return None

# ===================== íŒŒì¼/ì½ê¸°/ì§‘ê³„ =====================
def read_month_df(path: Path) -> pd.DataFrame:
    df = pd.read_excel(path, sheet_name="data", dtype=str)
    df = df.fillna("")
    # ìˆ«ìí˜•
    for c in ["ê³„ì•½ë…„","ê³„ì•½ì›”","ê³„ì•½ì¼","ê±°ë˜ê¸ˆì•¡(ë§Œì›)"]:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")
    return df

def eok_series(ser) -> pd.Series:
    s = pd.to_numeric(ser, errors="coerce").dropna()
    if s.empty: return pd.Series([], dtype=float)
    return s / 10000.0

def round2(v) -> str:
    try:
        return f"{float(v):.2f}"
    except Exception:
        return ""

def normalize_key(s: str) -> str:
    return re.sub(r"\s+","", str(s or "")).strip()

def agg_national_for_headers(df: pd.DataFrame, header: List[str]) -> Dict[str,int]:
    """
    ì›”ë³„ 'ì „êµ­' íƒ­ í—¤ë”(=ì§€ì—­ ì´ë¦„ë“¤)ì— ë§ì¶° í•©ê³„ë¥¼ ë½‘ì•„ ë„£ëŠ”ë‹¤.
    """
    counts: Dict[str,int] = {}
    if "ê´‘ì—­" not in df.columns:
        return {h:0 for h in header}

    # ê´‘ì—­ í‘œì¤€í™”
    prov_std = df["ê´‘ì—­"].map(lambda x: PROV_MAP.get(str(x), str(x)))
    grp = prov_std.groupby(prov_std).size()

    # í—¤ë”ì— ìˆëŠ” ê²ƒë§Œ ì±„ì›€ (ì—†ëŠ” ê±´ 0)
    for h in header:
        if not h or h == "ë‚ ì§œ" or h == "ì´í•©ê³„":
            continue
        v = 0
        if h in grp.index:
            v = int(grp[h])
        counts[h] = v
    return counts

def agg_seoul_for_headers(df: pd.DataFrame, header: List[str]) -> Dict[str,int]:
    """
    ì›”ë³„ 'ì„œìš¸' íƒ­ í—¤ë”(=ìì¹˜êµ¬ ì—´ë“¤)ì— ë§ì¶° ì„œìš¸-êµ¬ë³„ í•©ê³„ë¥¼ ë„£ëŠ”ë‹¤.
    """
    out: Dict[str,int] = {}
    if "ê´‘ì—­" not in df.columns or "êµ¬" not in df.columns:
        return {h:0 for h in header}

    se = df[normalize_key(df["ê´‘ì—­"]) == normalize_key("ì„œìš¸íŠ¹ë³„ì‹œ")]
    if se.empty:
        return {h:0 for h in header}

    grp = se["êµ¬"].map(lambda x: str(x).strip()).groupby(se["êµ¬"].map(lambda x: str(x).strip())).size()

    # í—¤ë”ì— ìˆëŠ” ê²ƒë§Œ ì±„ì›€ (ì—†ëŠ” ê±´ 0)
    for h in header:
        if not h or h == "ë‚ ì§œ" or h == "ì´í•©ê³„":
            continue
        v = 0
        if h in grp.index:
            v = int(grp[h])
        out[h] = v
    return out

def agg_all_stats(df: pd.DataFrame):
    """
    ê±°ë˜ìš”ì•½ íƒ­ìš©: SUMMARY_COLS ê¸°ì¤€ìœ¼ë¡œ ì „êµ­/ê´‘ì—­/ì„œìš¸/êµ¬/ì••êµ¬ì •ë™ì˜
    ê±´ìˆ˜, ì¤‘ì•™ê°’, í‰ê· ê°€(ì–µ)ë¥¼ ê³„ì‚°
    """
    counts = {col:0 for col in SUMMARY_COLS}
    med = {col:"" for col in SUMMARY_COLS}
    mean = {col:"" for col in SUMMARY_COLS}

    all_eok = eok_series(df.get("ê±°ë˜ê¸ˆì•¡(ë§Œì›)", []))
    counts["ì „êµ­"] = int(len(df))
    if not all_eok.empty:
        med["ì „êµ­"] = round2(all_eok.median())
        mean["ì „êµ­"] = round2(all_eok.mean())

    # ê´‘ì—­
    if "ê´‘ì—­" in df.columns:
        for prov, sub in df.groupby("ê´‘ì—­"):
            prov_std = PROV_MAP.get(str(prov), str(prov))
            if prov_std in counts:
                counts[prov_std] += int(len(sub))
                s = eok_series(sub.get("ê±°ë˜ê¸ˆì•¡(ë§Œì›)", []))
                if not s.empty:
                    med[prov_std] = round2(s.median())
                    mean[prov_std] = round2(s.mean())

    # ì„œìš¸/ìì¹˜êµ¬/ì••êµ¬ì •ë™
    seoul = df[normalize_key(df.get("ê´‘ì—­","")) == normalize_key("ì„œìš¸íŠ¹ë³„ì‹œ")].copy()
    counts["ì„œìš¸"] = int(len(seoul))
    if len(seoul)>0:
        s = eok_series(seoul["ê±°ë˜ê¸ˆì•¡(ë§Œì›)"])
        if not s.empty:
            med["ì„œìš¸"] = round2(s.median())
            mean["ì„œìš¸"] = round2(s.mean())

    if "êµ¬" in seoul.columns:
        for gu, sub in seoul.groupby("êµ¬"):
            gu = str(gu).strip()
            if gu in counts:
                counts[gu] += int(len(sub))
                s = eok_series(sub["ê±°ë˜ê¸ˆì•¡(ë§Œì›)"])
                if not s.empty:
                    med[gu] = round2(s.median())
                    mean[gu] = round2(s.mean())

    ap = seoul[seoul.get("ë²•ì •ë™","")=="ì••êµ¬ì •ë™"]
    counts["ì••êµ¬ì •ë™"] = int(len(ap))
    if len(ap)>0:
        s = eok_series(ap["ê±°ë˜ê¸ˆì•¡(ë§Œì›)"])
        med["ì••êµ¬ì •ë™"] = round2(s.median())
        mean["ì••êµ¬ì •ë™"] = round2(s.mean())

    return counts, med, mean

# ===================== ì›”ë³„ íƒ­ ê¸°ë¡ =====================
def kdate(d: datetime) -> str:
    return f"{d.year}. {d.month}. {d.day}"

def find_or_append_date_row(ws: gspread.Worksheet, date_label: str) -> int:
    vals = _retry(ws.get_all_values)
    if not vals:
        return 2
    for i, row in enumerate(vals[1:], start=2):
        if (len(row)>0) and str(row[0]).strip()==date_label:
            return i
    return len(vals)+1

def write_month_sheet(ws, date_label: str, header: List[str], values_by_colname: Dict[str,int]):
    hmap = {str(h).strip(): idx+1 for idx,h in enumerate(header) if str(h).strip()}
    row_idx = find_or_append_date_row(ws, date_label)
    payload = [{"range": f"A{row_idx}", "values": [[date_label]]}]

    # ê° ì—´ ì±„ìš°ê¸°
    for h in header:
        if not h or h=="ë‚ ì§œ":
            continue
        v = values_by_colname.get(h, 0)
        c = hmap.get(h)
        if c:
            payload.append({"range": f"{a1_col(c)}{row_idx}", "values": [[int(v)]]})

    # ì´í•©ê³„ê°€ ìˆìœ¼ë©´ ì „êµ­ í•©ê³„ ë˜ëŠ” í–‰ í•©ê³„ë¡œ ì±„ì›€(ë®ì–´ì“°ì§€ ì•ŠìŒ)
    if "ì´í•©ê³„" in hmap:
        v = values_by_colname.get("ì´í•©ê³„")
        if v is None:
            v = values_by_colname.get("ì „êµ­")
            if v is None:
                # í–‰ í•©ê³„ ê³„ì‚°
                v = 0
                for h in header:
                    if h and h not in ("ë‚ ì§œ","ì´í•©ê³„"):
                        v += int(values_by_colname.get(h, 0) or 0)
        payload.append({"range": f"{a1_col(hmap['ì´í•©ê³„'])}{row_idx}", "values": [[int(v)]]})

    if payload:
        values_batch_update(ws, payload)
        log(f"[ws] {ws.title} -> {date_label} row={row_idx}")

# ===================== ê±°ë˜ìš”ì•½ =====================
def ym_from_filename(fname: str):
    # 'ì „êµ­ 2509_250929.xlsx' â†’ ('ì „êµ­ 25ë…„ 9ì›”','ì„œìš¸ 25ë…„ 9ì›”','25/9')
    m = re.search(r"(\d{2})(\d{2})_", fname)
    if not m: return None, None, None
    yy, mm = m.group(1), int(m.group(2))
    return f"ì „êµ­ 20{yy}ë…„ {mm}ì›”", f"ì„œìš¸ 20{yy}ë…„ {mm}ì›”", f"{yy}/{mm}"

def prev_ym(ym: str) -> str:
    yy, mm = ym.split("/")
    y = int(yy); m = int(mm)
    if m == 1:
        return f"{y-1}/12"
    return f"{yy}/{m-1}"

def find_summary_row(ws, ym: str, label: str) -> int:
    vals = _retry(ws.get_all_values)
    if not vals:
        return 2
    for i, row in enumerate(vals[1:], start=2):
        a = str(row[0]).strip() if len(row)>0 else ""
        b = str(row[1]).strip() if len(row)>1 else ""
        if a==ym and b==label:
            return i
    return len(vals)+1

def put_summary_line(ws, row_idx: int, ym: str, label: str, line_map: dict):
    # ğŸš« ì•ˆì „ì¥ì¹˜: ìš”ì•½ íƒ­ì€ ë‚ ì§œ ë¬¸ìì—´(ì˜ˆ: '2025. 9. 29')ì„ ì“°ì§€ ì•ŠìŒ
    if "." in str(ym):
        log(f"[summary/guard] skip writing because ym looks like a date: {ym}")
        return

    header = _retry(ws.row_values, 1)
    if not header:
        _retry(ws.update, [["ë…„ì›”","êµ¬ë¶„"] + SUMMARY_COLS], "A1")
        header = ["ë…„ì›”","êµ¬ë¶„"] + SUMMARY_COLS
    hmap = {h:i+1 for i,h in enumerate(header)}
    payload = [
        {"range": f"A{row_idx}", "values": [[ym]]},
        {"range": f"B{row_idx}", "values": [[label]]},
    ]
    for c in SUMMARY_COLS:
        if c in hmap:
            payload.append({"range": f"{a1_col(hmap[c])}{row_idx}",
                            "values": [[line_map.get(c,"")]]})
    values_batch_update(ws, payload)

def color_diff_line(ws, row_idx: int, diff_line: dict, header: List[str]):
    hmap = {h:i+1 for i,h in enumerate(header)}
    reqs = []
    for k, v in diff_line.items():
        if k not in hmap: continue
        if v == "" or v == "0": continue
        r,g,b = (0.0,0.35,1.0) if str(v).startswith("+") else (1.0,0.0,0.0)
        reqs.append({
            "repeatCell": {
                "range": {
                    "sheetId": ws.id,
                    "startRowIndex": row_idx-1, "endRowIndex": row_idx,
                    "startColumnIndex": hmap[k]-1, "endColumnIndex": hmap[k]
                },
                "cell": {"userEnteredFormat":{"textFormat":{"foregroundColor":{"red":r,"green":g,"blue":b}}}}},
            "fields": "userEnteredFormat.textFormat.foregroundColor"
        })
    batch_format(ws, reqs)

def write_month_summary(ws, y: int, m: int, counts: dict, med: dict, mean: dict, prev_counts: Optional[dict]):
    ym = f"{str(y%100).zfill(2)}/{m}"

    r1 = find_summary_row(ws, ym, "ê±°ë˜ê±´ìˆ˜")
    put_summary_line(ws, r1, ym, "ê±°ë˜ê±´ìˆ˜", counts)
    log(f"[summary] {ym} ê±°ë˜ê±´ìˆ˜ -> row={r1}")

    r2 = find_summary_row(ws, ym, "ì¤‘ì•™ê°’(ë‹¨ìœ„:ì–µ)")
    put_summary_line(ws, r2, ym, "ì¤‘ì•™ê°’(ë‹¨ìœ„:ì–µ)", med)
    log(f"[summary] {ym} ì¤‘ì•™ê°’ -> row={r2}")

    r3 = find_summary_row(ws, ym, "í‰ê· ê°€(ë‹¨ìœ„:ì–µ)")
    put_summary_line(ws, r3, ym, "í‰ê· ê°€(ë‹¨ìœ„:ì–µ)", mean)
    log(f"[summary] {ym} í‰ê· ê°€ -> row={r3}")

    # ì „ì›”ëŒ€ë¹„ ê±´ìˆ˜ì¦ê° (+íŒŒë‘ / -ë¹¨ê°•)
    diffs = {}
    if prev_counts:
        for c in SUMMARY_COLS:
            cur = int(counts.get(c,0) or 0)
            prv = int(prev_counts.get(c,0) or 0)
            d = cur - prv
            diffs[c] = f"+{d}" if d>0 else (str(d) if d<0 else "0")
    else:
        diffs = {c:"" for c in SUMMARY_COLS}
    r4 = find_summary_row(ws, ym, "ì „ì›”ëŒ€ë¹„ ê±´ìˆ˜ì¦ê°")
    put_summary_line(ws, r4, ym, "ì „ì›”ëŒ€ë¹„ ê±´ìˆ˜ì¦ê°", diffs)
    header = _retry(ws.row_values, 1)
    color_diff_line(ws, r4, diffs, header)
    log(f"[summary] {ym} ì „ì›”ëŒ€ë¹„ -> row={r4}")

    # âœ… ìš”ì²­ì‚¬í•­: ì˜ˆìƒê±´ìˆ˜ëŠ” ë” ì´ìƒ ê¸°ë¡í•˜ì§€ ì•ŠìŒ

# ===================== ì••êµ¬ì •ë™ (ì›ë³¸ ê·¸ëŒ€ë¡œ + ë³€ë™ìš”ì•½) =====================
def _apgu_norm(v) -> str:
    return "" if v is None else str(v).strip()

def _apgu_key_from_row_values(values: List[str], header: List[str]) -> str:
    idx = {h:i for i,h in enumerate(header)}
    parts = []
    for h in APGU_BASE_COLS:
        i = idx.get(h, None)
        parts.append(_apgu_norm(values[i] if (i is not None and i < len(values)) else "")) 
    return "|".join(parts)

def _ensure_rows(ws: gspread.Worksheet, need_end_row: int):
    if need_end_row > ws.row_count:
        _retry(ws.add_rows, need_end_row - ws.row_count)

def fmt_kdate(d: date) -> str:
    return f"{d.year}. {d.month}. {d.day}"

def upsert_apgu_verbatim(ws: gspread.Worksheet, df_all: pd.DataFrame, run_day: date):
    df = df_all[df_all.get("ë²•ì •ë™","")=="ì••êµ¬ì •ë™"].copy()
    if df.empty:
        log("[ì••êµ¬ì •ë™] no rows")
        return

    # í•„ìˆ˜ ì»¬ëŸ¼ ì±„ìš°ê¸°
    for c in APGU_BASE_COLS:
        if c not in df.columns:
            df[c] = ""

    # ì˜¤ë˜ëœ â†’ ìµœì‹ 
    for c in ["ê³„ì•½ë…„","ê³„ì•½ì›”","ê³„ì•½ì¼"]:
        if c not in df.columns:
            df[c] = ""
    df = df.sort_values(["ê³„ì•½ë…„","ê³„ì•½ì›”","ê³„ì•½ì¼"], ascending=[True,True,True], kind="mergesort")

    # í—¤ë” ê³ ì •
    vals = _retry(ws.get_all_values) or []
    if not vals:
        _retry(ws.update, [APGU_BASE_COLS], "A1")
        vals = [APGU_BASE_COLS]
    header = vals[0]
    if header != APGU_BASE_COLS:
        _retry(ws.update, [APGU_BASE_COLS], "A1")
        header = APGU_BASE_COLS

    # ê¸°ì¡´ ë³¸í‘œ í–‰(ë³€ë™ ë¸”ë¡ ì œì™¸)
    all_now = _retry(ws.get_all_values) or [header]
    body = all_now[1:]
    base_rows_old: List[List[str]] = []
    for r in body:
        if r and r[0] in ("ë³€ê²½êµ¬ë¶„","(ì‹ ê·œ)","(ì‚­ì œ)"):
            break
        base_rows_old.append((r + [""]*len(header))[:len(header)])

    # ì˜¤ëŠ˜ ë³¸í‘œ (ì›ë³¸ ê·¸ëŒ€ë¡œ)
    base_rows_new: List[List[str]] = []
    for _, row in df.iterrows():
        base_rows_new.append([_apgu_norm(row.get(c, "")) for c in APGU_BASE_COLS])

    # ë³¸í‘œ ë®ì–´ì“°ê¸°
    start = 2
    end = start + len(base_rows_new) - 1
    if end < start: end = start
    _ensure_rows(ws, end)
    _retry(ws.update, base_rows_new, f"A{start}:{a1_col(len(header))}{end}")
    log(f"[ì••êµ¬ì •ë™] base rows written: {len(base_rows_new)}")

    # ì‹ ê·œ/ì‚­ì œ ë¹„êµ
    old_keys = {_apgu_key_from_row_values(r, header) for r in base_rows_old}
    new_keys = {_apgu_key_from_row_values(r, header) for r in base_rows_new}
    added_keys = sorted(list(new_keys - old_keys))
    removed_keys = sorted(list(old_keys - new_keys))

    if not added_keys and not removed_keys:
        log("[ì••êµ¬ì •ë™] changes: none")
        return

    def _rowmap(rows: List[List[str]]):
        m={}
        for r in rows:
            m[_apgu_key_from_row_values(r, header)] = r
        return m

    new_map = _rowmap(base_rows_new)
    old_map = _rowmap(base_rows_old)

    change_header = ["ë³€ê²½êµ¬ë¶„","ë³€ê²½ì¼"] + APGU_BASE_COLS
    change_rows: List[List[str]] = [change_header]
    today_str = fmt_kdate(run_day)
    for k in added_keys:
        change_rows.append(["(ì‹ ê·œ)", today_str] + new_map[k])
    for k in removed_keys:
        change_rows.append(["(ì‚­ì œ)", today_str] + old_map[k])

    start_chg = end + 1
    end_chg = start_chg + len(change_rows) - 1
    _ensure_rows(ws, end_chg)
    _retry(ws.update, change_rows, f"A{start_chg}:{a1_col(len(change_header))}{end_chg}")

    # ë¹¨ê°„ ê¸€ì”¨
    req = {
        "repeatCell": {
            "range": {
                "sheetId": ws.id,
                "startRowIndex": start_chg-1,
                "endRowIndex": end_chg,
                "startColumnIndex": 0,
                "endColumnIndex": len(change_header)
            },
            "cell": {"userEnteredFormat":{"textFormat":{"foregroundColor":{"red":1.0,"green":0.0,"blue":0.0}}}}},
        "fields": "userEnteredFormat.textFormat.foregroundColor"
    }
    batch_format(ws, [req])
    log(f"[ì••êµ¬ì •ë™] changes: ì‹ ê·œ={len(added_keys)} ì‚­ì œ={len(removed_keys)}")

# ===================== ë©”ì¸ =====================
def main():
    # ë¡œê·¸ ì´ˆê¸°í™”
    try: RUN_LOG.write_text("", encoding="utf-8")
    except Exception: pass

    log("[MAIN]")
    log(f"artifacts_dir={ARTIFACTS_DIR}")

    # ì¸ì¦
    sa_json = os.environ.get("SA_JSON","").strip()
    sa_path = os.environ.get("SA_PATH","sa.json")
    if sa_json:
        creds = Credentials.from_service_account_info(
            json.loads(sa_json),
            scopes=["https://www.googleapis.com/auth/spreadsheets","https://www.googleapis.com/auth/drive"],
        )
    else:
        if not Path(sa_path).exists():
            raise RuntimeError("service account not provided")
        creds = Credentials.from_service_account_file(
            sa_path,
            scopes=["https://www.googleapis.com/auth/spreadsheets","https://www.googleapis.com/auth/drive"],
        )
    gc = gspread.authorize(creds)
    sh = _retry(gc.open_by_key, os.environ.get("SHEET_ID","").strip())
    log("[gspread] spreadsheet opened")

    # íŒŒì¼ ìˆ˜ì§‘
    files = sorted(Path(ARTIFACTS_DIR).rglob("ì „êµ­ *.xlsx"))
    log(f"[collect] found {len(files)} xlsx files")

    month_cache = {}  # ym -> {counts, med, mean}
    today_label = kdate(datetime.now())
    run_day = datetime.now().date()

    apgu_all: List[pd.DataFrame] = []

    for p in files:
        nat_title, se_title, ym = ym_from_filename(p.name)
        if not ym:
            continue
        log(f"[file] {p.name} -> nat='{nat_title}' / seoul='{se_title}' / ym={ym}")

        df = read_month_df(p)
        log(f"[read] rows={len(df)} cols={len(df.columns)}")

        counts, med, mean = agg_all_stats(df)
        month_cache[ym] = {"counts": counts, "med": med, "mean": mean}

        # ì›”ë³„ íƒ­ ê¸°ë¡ (ìˆëŠ” íƒ­ì—ë§Œ) â€” ê° íƒ­ì˜ í—¤ë”ëª…ì„ ê¸°ì¤€ìœ¼ë¡œ ì§‘ê³„
        ws_nat = fuzzy_ws(sh, nat_title) or fuzzy_ws(sh, nat_title.replace("20","",1))
        if ws_nat:
            header_nat = _retry(ws_nat.row_values, 1)
            nat_counts = agg_national_for_headers(df, header_nat)
            # ì´í•©ê³„ëŠ” write_month_sheet ë‚´ë¶€ì—ì„œ ë³´ì •
            write_month_sheet(ws_nat, today_label, header_nat, nat_counts)

        ws_se = fuzzy_ws(sh, se_title) or fuzzy_ws(sh, se_title.replace("20","",1))
        if ws_se:
            header_se = _retry(ws_se.row_values, 1)
            se_counts = agg_seoul_for_headers(df, header_se)
            # ì´í•©ê³„ëŠ” write_month_sheet ë‚´ë¶€ì—ì„œ ë³´ì •(ì„œìš¸ í•©ê³„ ìë™ ê³„ì‚°)
            write_month_sheet(ws_se, today_label, header_se, se_counts)

        # ì••êµ¬ì •ë™ ì›ë³¸ ëˆ„ì 
        ap = df[(normalize_key(df.get("ê´‘ì—­","")) == normalize_key("ì„œìš¸íŠ¹ë³„ì‹œ")) &
                (df.get("ë²•ì •ë™","")=="ì••êµ¬ì •ë™")].copy()
        if not ap.empty:
            apgu_all.append(ap)

    # ê±°ë˜ìš”ì•½
    ws_sum = fuzzy_ws(sh, SUMMARY_SHEET_NAME)
    if ws_sum and month_cache:
        def ym_key(ym): 
            yy, mm = ym.split("/")
            return (int(yy), int(mm))
        for ym in sorted(month_cache.keys(), key=ym_key):
            cur = month_cache[ym]
            prv = month_cache.get(prev_ym(ym))
            write_month_summary(
                ws_sum,
                2000 + int(ym.split("/")[0]),
                int(ym.split("/")[1]),
                cur["counts"], cur["med"], cur["mean"],
                prv["counts"] if prv else None
            )
            time.sleep(0.2)

    # ì••êµ¬ì •ë™ ì›ë³¸ ê·¸ëŒ€ë¡œ + ë³€ë™ìš”ì•½
    if apgu_all:
        ws_ap = fuzzy_ws(sh, "ì••êµ¬ì •ë™")
        if ws_ap:
            all_df = pd.concat(apgu_all, ignore_index=True)
            upsert_apgu_verbatim(ws_ap, all_df, run_day)
        else:
            log("[ì••êµ¬ì •ë™] sheet not found (skip)")

    log("[main] done")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        log(f"[ERROR] {repr(e)}")
        raise
